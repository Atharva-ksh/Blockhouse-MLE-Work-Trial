# -*- coding: utf-8 -*-
"""Blockhouse_Code_MLE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11PlfX54PRp9NJCsFYgBmQ9y0bww40Fv9

# Get the data
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

data = pd.read_csv('/content/drive/MyDrive/Blockhouse-Data/xnas-itch-20230703.tbbo.csv')
data.head()

data.describe()

data.info()

from google.colab import drive
drive.mount('/content/drive')

"""# Add Technical Identifiers to this data

Technical indicators created for the environment include:

- RSI
- MACD
- Stoch_k
- OBV
- Upper_BB
- ATR_1
- ATR_2
- ATR_5
- ATR_10
- ATR_20
"""

url = 'https://anaconda.org/conda-forge/libta-lib/0.4.0/download/linux-64/libta-lib-0.4.0-h166bdaf_1.tar.bz2'
!curl -L $url | tar xj -C /usr/lib/x86_64-linux-gnu/ lib --strip-components=1
url = 'https://anaconda.org/conda-forge/ta-lib/0.4.19/download/linux-64/ta-lib-0.4.19-py310hde88566_4.tar.bz2'
!curl -L $url | tar xj -C /usr/local/lib/python3.10/dist-packages/ lib/python3.10/site-packages/talib --strip-components=3

import pandas as pd
import numpy as np
import talib as ta

class TechnicalIndicators:
    def __init__(self, data):
        self.data = data

    def add_momentum_indicators(self):
        self.data['RSI'] = ta.RSI(self.data['Close'], timeperiod=14)
        self.data['MACD'], self.data['MACD_signal'], self.data['MACD_hist'] = ta.MACD(self.data['Close'], fastperiod=12, slowperiod=26, signalperiod=9)
        self.data['Stoch_k'], self.data['Stoch_d'] = ta.STOCH(self.data['High'], self.data['Low'], self.data['Close'],
                                                              fastk_period=14, slowk_period=3, slowd_period=3)
        # Adding Rate of Change to the technical indicators which measures % change between current and old prices
        self.data['ROC'] = ta.ROC(self.data['Close'], timeperiod=10)

    def add_volume_indicators(self):
        self.data['OBV'] = ta.OBV(self.data['Close'], self.data['Volume'])

    def add_volatility_indicators(self):
        self.data['Upper_BB'], self.data['Middle_BB'], self.data['Lower_BB'] = ta.BBANDS(self.data['Close'], timeperiod=20)
        self.data['ATR_1'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=1)
        self.data['ATR_2'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=2)
        self.data['ATR_5'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=5)
        self.data['ATR_10'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=10)
        self.data['ATR_20'] = ta.ATR(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=20)

    def add_trend_indicators(self):
        self.data['ADX'] = ta.ADX(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=14)
        self.data['+DI'] = ta.PLUS_DI(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=14)
        self.data['-DI'] = ta.MINUS_DI(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=14)
        self.data['CCI'] = ta.CCI(self.data['High'], self.data['Low'], self.data['Close'], timeperiod=5)

    def add_other_indicators(self):
        self.data['DLR'] = np.log(self.data['Close'] / self.data['Close'].shift(1))
        self.data['TWAP'] = self.data['Close'].expanding().mean()
        self.data['VWAP'] = (self.data['Volume'] * (self.data['High'] + self.data['Low']) / 2).cumsum() / self.data['Volume'].cumsum()

    def add_all_indicators(self):
        self.add_momentum_indicators()
        self.add_volume_indicators()
        self.add_volatility_indicators()
        self.add_trend_indicators()
        self.add_other_indicators()
        return self.data

# Preprocessing to create necessary columns
data['price']=data['price']/1e9
data['bid_px_00']=data['bid_px_00']/1e9
data['ask_px_00']=data['ask_px_00']/1e9

data['Close'] = data['price']
data['Volume'] = data['size']
data['High'] = data[['bid_px_00', 'ask_px_00']].max(axis=1)
data['Low'] = data[['bid_px_00', 'ask_px_00']].min(axis=1)
data['Open'] = data['Close'].shift(1).fillna(data['Close'])


ti = TechnicalIndicators(data)
df_with_indicators = ti.add_all_indicators()
market_features_df = df_with_indicators[35:]

"""Check the dataset:"""

# Show all columns in pandas
pd.set_option('display.max_columns', None)

market_features_df.head(35)

market_features_df.to_csv('market_features_df.csv')

"""# Trading Environement Class for the Transformer Model"""

import gym
from gym import spaces
import numpy as np
import pandas as pd

class TradingEnvironment(gym.Env):
    metadata = {'render.modes': ['human']}

    def __init__(self, data, daily_trading_limit):
        super(TradingEnvironment, self).__init__()
        self.data = data
        self.daily_trading_limit = daily_trading_limit
        self.current_step = 0

        # Extract state columns
        self.state_columns = ['Close', 'Volume', 'RSI', 'MACD', 'MACD_signal', 'MACD_hist', 'Stoch_k', 'Stoch_d',
                              'OBV', 'Upper_BB', 'Middle_BB', 'Lower_BB', 'ATR_1', 'ADX', '+DI', '-DI', 'CCI', 'ROC']

        # Initialize balance, shares held, and total shares traded
        self.balance = 10_000_000.0  # $10 million
        self.shares_held = 0
        self.total_shares_traded = 0

        # Define action space: [Hold, Buy, Sell]
        self.action_space = spaces.Discrete(3)

        # Define observation space based on state columns
        self.observation_space = spaces.Box(
            low=-np.inf, high=np.inf, shape=(len(self.state_columns),), dtype=np.float32
        )

    def reset(self):
        self.current_step = 0
        self.balance = 10_000_000.0  # $10 million
        self.shares_held = 0
        self.total_shares_traded = 0
        self.cumulative_reward = 0
        self.trades = []
        return self._next_observation()

    def _next_observation(self):
        return self.data[self.state_columns].iloc[self.current_step].values

    def step(self, action):
        expected_price = self.data.iloc[self.current_step]['ask_px_00']
        actual_price = self.data.iloc[self.current_step]['price']
        transaction_time = self.data.iloc[self.current_step]['ts_in_delta']
        self._take_action(action)
        reward = 0

        # if self.current_step >= len(self.data) - 1:
        #     self.current_step = 0

        if action != 0:
            transaction_cost= self._calculate_transaction_cost(self.data.iloc[self.current_step]['Volume'], 0.3, self.data['Volume'].mean())
            reward = self._calculate_reward(expected_price, actual_price, transaction_time, transaction_cost)
            self.cumulative_reward += reward
            if self.trades:
                self.trades[-1]['reward'] = reward
                self.trades[-1]['transaction_cost'] = transaction_cost
                self.trades[-1]['slippage'] = expected_price - actual_price
                self.trades[-1]['time_penalty'] = 100*transaction_time/1e9
        done = self.current_step == len(self.data) - 1
        obs = self._next_observation()
        info = {
        'step': self.current_step,
        'action': action,
        'price': actual_price,
        'shares': self.trades[-1]['shares'] if self.trades else 0
    }
        if not done:
          self.current_step += 1

        return obs, reward, done, info

    def _take_action(self, action):
        current_price = self.data.iloc[self.current_step]['Close']
        current_time = pd.to_datetime(self.data.iloc[self.current_step]['ts_event'])
        trade_info = {'step': self.current_step, 'timestamp': current_time, 'action': action, 'price': current_price, 'shares': 0, 'reward': 0, 'transaction_cost': 0, 'slippage': 0, 'time_penalty': 0}

        if action == 1: # and self.total_shares_traded < self.daily_trading_limit:  # Buy
            shares_bought = (self.balance * np.random.uniform(0.001, 0.005)) // current_price
            self.balance -= shares_bought * current_price
            self.shares_held += shares_bought
            self.total_shares_traded += shares_bought
            trade_info['shares'] = shares_bought
            if(shares_bought>0):
                self.trades.append(trade_info)
        elif action == 2: # and self.total_shares_traded < self.daily_trading_limit:  # Sell
            shares_sold = min((self.balance * np.random.uniform(0.001, 0.005)) // current_price, self.shares_held)
            self.balance += shares_sold * current_price
            self.shares_held -= shares_sold
            self.total_shares_traded -= shares_sold
            trade_info['shares'] = shares_sold
            if(shares_sold>0):
                self.trades.append(trade_info)

    def _calculate_reward(self, expected_price, actual_price, transaction_time, transaction_cost):
        slippage = expected_price - actual_price
        time_penalty = 100*transaction_time/1e9
        reward = - (slippage + time_penalty + transaction_cost)
        return reward

    def _calculate_transaction_cost(self, volume, volatility, daily_volume):
        return volatility * np.sqrt(volume / daily_volume)

    def run(self):
        self.reset()
        for _ in range(len(self.data)):
            self.step()
        return self.cumulative_reward, self.trades

    def render(self, mode='human', close=False):
        print(f'Step: {self.current_step}')
        print(f'Balance: {self.balance}')
        print(f'Shares held: {self.shares_held}')
        print(f'Total shares traded: {self.total_shares_traded}')
        print(f'Total portfolio value: {self.balance + self.shares_held * self.data.iloc[self.current_step]["Close"]}')
        print(f'Cumulative reward: {self.cumulative_reward}')
        self.print_trades()

    def print_trades(self):
        # download all trades in a pandas dataframe using .csv
        trades_df = pd.DataFrame(self.trades)
        # Save a csv
        trades_df.to_csv('trades_ppo.csv', index=False)
        for trade in self.trades:
            print(f"Step: {trade['step']}, Timestamp: {trade['timestamp']}, Action: {trade['action']}, Price: {trade['price']}, Shares: {trade['shares']}, Reward: {trade['reward']}, Transaction Cost: {trade['transaction_cost']}, Slippage: {trade['slippage']}, Time Penalty: {trade['time_penalty']}")

"""# Defining Transformer Architecture with LSTM

I have designed the architecture as follows:
1. Transformer Encoder layer
2. LSTM Layer
3. Action Output

The Transformer I am using is BERT which is Bidirectional Encoder. I am fine-tuning the last 2 layers of the BERT model and have added MultiHead attention layers to it as well. So the architecture of the model is such:


BERT Model -> Multihead Attention Layer -> LSTM Layer -> Output (Hold, Buy, Sell)
"""

# Install required packages
!pip install transformers torch stable-baselines3 gym numpy pandas

import torch
import torch.nn as nn
import torch.optim as optim
from transformers import BertModel, AdamW

class TransformerModel(nn.Module):
  def __init__(self, bert_model_name, num_attention_heads=4, attention_dropouts=0.1, lstm_hidden_size=128, lstm_layers=1):
    super(TransformerModel, self).__init__()
    self.bert_model = BertModel.from_pretrained(bert_model_name)

    # Freezing all layers in Bert
    for param in self.bert_model.parameters():
      param.requires_grad = False
    # Unfreezing last two layers for finetuning
    for param in self.bert_model.encoder.layer[-2:].parameters():
      param.requires_grad = True
    self.multihead_attention = nn.MultiheadAttention(embed_dim=self.bert_model.config.hidden_size, num_heads=num_attention_heads, dropout=attention_dropouts)
    self.lstm = nn.LSTM(input_size=self.bert_model.config.hidden_size, hidden_size=lstm_hidden_size, num_layers=lstm_layers, batch_first=True)
    self.classifier = nn.Linear(lstm_hidden_size, 3)

  def forward(self, input_ids, attention_mask):
    bert_output = self.bert_model(input_ids=input_ids, attention_mask=attention_mask)
    last_hidden_state = bert_output.last_hidden_state
    last_hidden_state = last_hidden_state.transpose(0,1)

    attention_output, _ = self.multihead_attention(query=last_hidden_state, key=last_hidden_state, value=last_hidden_state)
    attention_output = attention_output.transpose(0,1)

    lstm_output, _ = self.lstm(attention_output)
    lstm_output = lstm_output[:, -1, :]

    logits = self.classifier(lstm_output)
    return logits

# Test dataset with smaller # of datapoints
small_dataset = market_features_df.iloc[:25000,:]
small_dataset.head()
print(len(small_dataset))

# Initialize the environment
env = TradingEnvironment(market_features_df, 100)

"""# Initialize the model and hyperparameters"""

# Hyperparameters for training
learning_rate = 1e-3
batch_size = 16
num_epochs = 1
num_attention_heads = 8
attention_dropout = 0.2
lstm_hidden_size = 128
lstm_layers = 2
update_frequency = 50

bert_model_name = 'bert-base-uncased'
model = TransformerModel(bert_model_name, num_attention_heads, attention_dropout, lstm_hidden_size, lstm_layers)
model.train()

optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)

"""# Training Loop for the model

# Iteration #1A

# Baseline Model with a simple loss function

Whats this?
- This is a vanilla model with a simple loss function. The aim of this model is to understand model behaviour and introduce updates in this model.

Results:
- Model did not complete the training loop because of RAM constraint
"""

for epoch in range(num_epochs):
    state = env.reset()
    done = False
    cumulative_reward = 0

    while not done:
        input_ids = torch.tensor(state, dtype=torch.long).unsqueeze(0)
        input_ids = torch.clamp(input_ids, 0, model.bert_model.config.vocab_size - 1)  # Clamp values
        attention_mask = torch.ones_like(input_ids)

        logits = model(input_ids, attention_mask)
        action_probs = torch.softmax(logits, dim=1)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()

        action = action.item()
        print(f"Action taken: {action}")

        next_state, reward, done, _ = env.step(action)
        cumulative_reward += reward

        state = next_state

    # Update policy with simple loss function
    optimizer.zero_grad()
    loss = -cumulative_reward
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}/{num_epochs}, Cumulative Reward: {cumulative_reward}")

"""This is the training loop being used to fine tune the model for predicting reliable trade recommendations.

I am using Reinforcement Learning Technique to finetune the model as it is mostly an unsupervised learning task. The model is finetuned by using rewards structure where rewards are calculated by the trading environment. These rewards are then used to optimize the model by minimizing the loss function which is calculated with log probability and discounted reward which prioritizes quick rewards over long term rewards.

The update is run at an iteration of 50 steps so the model learns relatively quickly while saving RAM.

# Iteration #1B

# Using Discounted Rewards Loss with a buffer to manage RAM issues

Whats New?
- Implemented standard RL Experience Replay Buffer
- Introduced a smarter loss function

Results:
- Completed the training loop
- High bias towards 0 (HOLD)
"""

from collections import deque
import random
import gc

class ExperienceReplay:
    def __init__(self, capacity):
        self.buffer = deque(maxlen=capacity)

    def push(self, experience):
        self.buffer.append(experience)

    def sample(self, batch_size):
        return random.sample(self.buffer, batch_size)

    def __len__(self):
        return len(self.buffer)

# Initialize replay buffer
replay_buffer = ExperienceReplay(capacity=10000)

for epoch in range(num_epochs):
    state = env.reset()
    done = False
    cumulative_reward = 0
    steps = 0

    while not done:
        input_ids = torch.tensor(state, dtype=torch.long).unsqueeze(0)
        input_ids = torch.clamp(input_ids, 0, model.bert_model.config.vocab_size - 1)
        attention_mask = torch.ones_like(input_ids)
        print('Input:',input_ids)

        logits = model(input_ids, attention_mask)
        action_probs = torch.softmax(logits, dim=1)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()

        log_prob = action_dist.log_prob(action)
        action = action.item()
        print('Action:',action)
        next_state, reward, done, _ = env.step(action)
        cumulative_reward += reward

        replay_buffer.push((state, action, reward, next_state, done))

        state = next_state
        steps += 1

        # Update the model periodically
        if len(replay_buffer) > batch_size and steps % update_frequency == 0:
            experiences = replay_buffer.sample(batch_size)
            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)

            batch_states = torch.tensor(batch_states, dtype=torch.long)
            batch_states = torch.clamp(batch_states, 0, model.bert_model.config.vocab_size - 1)
            batch_actions = torch.tensor(batch_actions, dtype=torch.long)
            batch_rewards = torch.tensor(batch_rewards, dtype=torch.float)
            batch_next_states = torch.tensor(batch_next_states, dtype=torch.long)
            batch_dones = torch.tensor(batch_dones, dtype=torch.float)

            optimizer.zero_grad()
            logits = model(batch_states, attention_mask=torch.ones_like(batch_states))
            log_probs = torch.log_softmax(logits, dim=1)
            selected_log_probs = log_probs[range(batch_size), batch_actions]

            # Calculate discounted rewards
            discounted_rewards = []
            cumulative = 0
            for reward in reversed(batch_rewards):
                cumulative = reward + 0.99 * cumulative
                discounted_rewards.insert(0, cumulative)
            discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float)

            loss = -torch.mean(selected_log_probs * discounted_rewards)
            loss.backward()
            optimizer.step()

        # Free up memory and clear cache
        if steps % 100 == 0:
            torch.cuda.empty_cache()
            gc.collect()

    print(f"Epoch {epoch+1}/{num_epochs}, Cumulative Reward: {cumulative_reward}")

env.render()

"""- I made an observation when I got the results above. It seems that as the algorithm goes over all the 59k data points, it tends to become stagnant for action 0 and prefers 'Holding' the stock over 'Buying' or 'Selling'. This could be because the reward structure for Holding does not make any change in the final rewards and Buying and Selling are more risky and hence more rewarding as well.

Why Use Entropy Regularization?
- Encourages Exploration: By penalizing low-entropy (deterministic) policies, the model is encouraged to explore more diverse actions.
- Prevents Premature Convergence: It helps prevent the model from prematurely converging to a suboptimal policy.
- Balances Exploration and Exploitation: It inherently balances exploration and exploitation by making the policy more stochastic.

# Iteration #2

# Entropy Regularization to combat bias towards 0 (HOLD)

Whats New?
- Implemented Entropy Regularization

Results:
- Model tries to explore other actions.
- In later stages of training, the model is biased towards 0 again.

I am using Experience Replay for reducing the workload on RAM. Experience Replay is basically a buffering technique by which I can update the model more frequently and sample the experiences for training purposes. It also helps with clearning cache and clearing garbage values.
"""

initial_epsilon = 0.1  # Initial exploration probability
epsilon_decay = 0.995  # Decay rate for epsilon
min_epsilon = 0.01  # Minimum value for epsilon
entropy_beta = 0.01  # Entropy regularization coefficient
epsilon = initial_epsilon

for epoch in range(num_epochs):
    state = env.reset()
    done = False
    cumulative_reward = 0
    steps = 0

    while not done:
        input_ids = torch.tensor(state, dtype=torch.long).unsqueeze(0)
        input_ids = torch.clamp(input_ids, 0, model.bert_model.config.vocab_size - 1)
        attention_mask = torch.ones_like(input_ids)
        print('Step:',steps,' Input:',input_ids)

        logits = model(input_ids, attention_mask)
        action_probs = torch.softmax(logits, dim=1)
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample()

        log_prob = action_dist.log_prob(action)
        action = action.item()


        print('Action:',action)
        next_state, reward, done, _ = env.step(action)

        print('Reward:',reward)
        cumulative_reward += reward

        replay_buffer.push((state, action, reward, next_state, done))

        state = next_state
        steps += 1

        if len(replay_buffer) > batch_size and steps % update_frequency == 0:
            print('Updating the model at step: ',steps)
            experiences = replay_buffer.sample(batch_size)
            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)

            batch_states = torch.tensor(batch_states, dtype=torch.long)
            batch_states = torch.clamp(batch_states, 0, model.bert_model.config.vocab_size - 1)
            batch_actions = torch.tensor(batch_actions, dtype=torch.long)
            batch_rewards = torch.tensor(batch_rewards, dtype=torch.float)
            batch_next_states = torch.tensor(batch_next_states, dtype=torch.long)
            batch_dones = torch.tensor(batch_dones, dtype=torch.float)

            optimizer.zero_grad()
            logits = model(batch_states, attention_mask=torch.ones_like(batch_states))
            log_probs = torch.log_softmax(logits, dim=1)
            selected_log_probs = log_probs[range(batch_size), batch_actions]

            discounted_rewards = []
            cumulative = 0
            for reward in reversed(batch_rewards):
                cumulative = reward + 0.99 * cumulative
                discounted_rewards.insert(0, cumulative)
            discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float)

            # Calculate entropy for regularization
            entropy = -torch.sum(action_probs * log_probs, dim=1).mean()

            loss = -torch.mean(selected_log_probs * discounted_rewards) - entropy_beta * entropy
            loss.backward()
            optimizer.step()

        if steps % 100 == 0:
            torch.cuda.empty_cache()
            gc.collect()

    print(f"Epoch {epoch+1}/{num_epochs}, Cumulative Reward: {cumulative_reward}")
    # Epsilon decay
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

env.render()

"""# Iteration #3

# Adding Epsilon-Greedy Strategy

Whats New?
- Added simple buffer instead of Experience Replay Buffer
- Implemented decaying Epsilon-Greedy Strategy

Results:
- Better participation in trades: Traded 49414 stocks
- Better sequential understanding with a simple buffer
"""

# Using a simple bufffer to hold experiences

class SimpleBuffer:
    def __init__(self, capacity):
        self.capacity = capacity
        self.buffer = []

    def push(self, experience):
        if len(self.buffer) >= self.capacity:
            self.buffer.pop(0)
        self.buffer.append(experience)

    def get_all(self):
        return self.buffer

    def clear(self):
        self.buffer = []

    def __len__(self):
        return len(self.buffer)

# Initialize the simple buffer
batch_size = 50
buffer_capacity = batch_size
buffer = SimpleBuffer(capacity=buffer_capacity)

import random
import gc


# Hyperparameters
initial_epsilon = 0.1
epsilon_decay = 0.995
min_epsilon = 0.01
entropy_beta = 0.01
epsilon = initial_epsilon
update_frequency = batch_size

for epoch in range(num_epochs):
    state = env.reset()
    done = False
    cumulative_reward = 0
    steps = 0

    while not done:
        input_ids = torch.tensor(state, dtype=torch.long).unsqueeze(0)
        input_ids = torch.clamp(input_ids, 0, model.bert_model.config.vocab_size - 1)
        attention_mask = torch.ones_like(input_ids)
        print('Step:', steps, ' Input:', input_ids)

        logits = model(input_ids, attention_mask)
        action_probs = torch.softmax(logits, dim=1)

        # Epsilon-greedy strategy for exploration
        if random.random() < epsilon:
            action = np.random.choice([0, 1, 2])  # Explore
        else:
            action_dist = torch.distributions.Categorical(action_probs)
            action = action_dist.sample().item()  # Exploit

        print('Action:', action)
        next_state, reward, done, _ = env.step(action)
        print('Reward:', reward)
        cumulative_reward += reward

        buffer.push((state, action, reward, next_state, done))

        state = next_state
        steps += 1

        # Update the model when the buffer is full
        if len(buffer) >= buffer_capacity:
            print('Updating the model at step: ', steps)
            experiences = buffer.get_all()
            batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)

            batch_states = torch.tensor(batch_states, dtype=torch.long)
            batch_states = torch.clamp(batch_states, 0, model.bert_model.config.vocab_size - 1)
            batch_actions = torch.tensor(batch_actions, dtype=torch.long)
            batch_rewards = torch.tensor(batch_rewards, dtype=torch.float)
            batch_next_states = torch.tensor(batch_next_states, dtype=torch.long)
            batch_dones = torch.tensor(batch_dones, dtype=torch.float)

            optimizer.zero_grad()
            logits = model(batch_states, attention_mask=torch.ones_like(batch_states))
            log_probs = torch.log_softmax(logits, dim=1)
            selected_log_probs = log_probs[torch.arange(len(batch_states)), batch_actions]

            discounted_rewards = []
            cumulative = 0
            for reward in reversed(batch_rewards):
                cumulative = reward + 0.99 * cumulative
                discounted_rewards.insert(0, cumulative)
            discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float)
            discounted_rewards = discounted_rewards[:len(selected_log_probs)]

            entropy = -torch.sum(action_probs * log_probs, dim=1).mean()

            loss = -torch.mean(selected_log_probs * discounted_rewards) - entropy_beta * entropy
            loss.backward()
            optimizer.step()
            buffer.clear()

        # Free up memory and clear cache
        if steps % 100 == 0:
            torch.cuda.empty_cache()
            gc.collect()

    print(f"Epoch {epoch+1}/{num_epochs}, Cumulative Reward: {cumulative_reward}")
    # Epsilon decay
    epsilon = max(min_epsilon, epsilon * epsilon_decay)

env.render()

"""# Epsilon-Greedy Strategy with Early Stopping"""

import random
import gc


# Hyperparameters
initial_epsilon = 0.1
epsilon_decay = 0.995
min_epsilon = 0.01
entropy_beta = 0.01
epsilon = initial_epsilon
update_frequency = batch_size

# Early Stopping Parameters
early_stopping_patience = 10  # Number of intervals to wait for improvement
early_stopping_counter = 0
best_cumulative_reward = -float('inf')
check_interval = 1000  # Check improvement every 1000 steps

state = env.reset()
done = False
cumulative_reward = 0
steps = 0

while not done:
    input_ids = torch.tensor(state, dtype=torch.long).unsqueeze(0)
    input_ids = torch.clamp(input_ids, 0, model.bert_model.config.vocab_size - 1)
    attention_mask = torch.ones_like(input_ids)
    print('Step:', steps, ' Input:', input_ids)

    logits = model(input_ids, attention_mask)
    action_probs = torch.softmax(logits, dim=1)

    # Epsilon-greedy strategy for exploration
    if random.random() < epsilon:
        action = np.random.choice([0, 1, 2])
    else:
        action_dist = torch.distributions.Categorical(action_probs)
        action = action_dist.sample().item()

    print('Action:', action)
    next_state, reward, done, _ = env.step(action)
    print('Reward:', reward)
    cumulative_reward += reward

    buffer.push((state, action, reward, next_state, done))

    state = next_state
    steps += 1

    if len(buffer) >= buffer_capacity:
        print('Updating the model at step: ', steps)
        experiences = buffer.get_all()
        batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = zip(*experiences)

        batch_states = torch.tensor(batch_states, dtype=torch.long)
        batch_states = torch.clamp(batch_states, 0, model.bert_model.config.vocab_size - 1)
        batch_actions = torch.tensor(batch_actions, dtype=torch.long)
        batch_rewards = torch.tensor(batch_rewards, dtype=torch.float)
        batch_next_states = torch.tensor(batch_next_states, dtype=torch.long)
        batch_dones = torch.tensor(batch_dones, dtype=torch.float)

        optimizer.zero_grad()
        logits = model(batch_states, attention_mask=torch.ones_like(batch_states))
        log_probs = torch.log_softmax(logits, dim=1)
        selected_log_probs = log_probs[torch.arange(len(batch_states)), batch_actions]

        discounted_rewards = []
        cumulative = 0
        for reward in reversed(batch_rewards):
            cumulative = reward + 0.99 * cumulative
            discounted_rewards.insert(0, cumulative)
        discounted_rewards = torch.tensor(discounted_rewards, dtype=torch.float)
        discounted_rewards = discounted_rewards[:len(selected_log_probs)]

        entropy = -torch.sum(action_probs * log_probs, dim=1).mean()

        loss = -torch.mean(selected_log_probs * discounted_rewards) - entropy_beta * entropy
        loss.backward()
        optimizer.step()
        buffer.clear()

    if steps % 100 == 0:
        torch.cuda.empty_cache()
        gc.collect()

    # Check for early stopping at regular intervals
    if steps % check_interval == 0:
        print(f"Check Interval: {steps}, Cumulative Reward: {cumulative_reward}")
        if cumulative_reward > best_cumulative_reward:
            best_cumulative_reward = cumulative_reward
            early_stopping_counter = 0  # Reset if improvement
        else:
            early_stopping_counter += 1

        if early_stopping_counter >= early_stopping_patience:
            print("Early stopping triggered")
            break

print(f"Total Steps: {steps}, Cumulative Reward: {cumulative_reward}")

env.render()

"""# Evaluating the model (v3 with early stopping)"""

model.eval()
total_rewards = []
env_eval = TradingEnvironment(market_features_df, 100)
state = env_eval.reset()
done = False
cumulative_reward = 0
step = 0

while not done:
    input_ids = torch.tensor(state, dtype=torch.long).unsqueeze(0)
    input_ids = torch.clamp(input_ids, 0, model.bert_model.config.vocab_size - 1)
    attention_mask = torch.ones_like(input_ids)

    with torch.no_grad():
        logits = model(input_ids, attention_mask)
        action_probs = torch.softmax(logits, dim=1)
        action = torch.argmax(action_probs, dim=1).item()

    next_state, reward, done, _ = env_eval.step(action)
    cumulative_reward += reward
    state = next_state
    step += 1

env_eval.render()

"""## This concludes our testing. I feel there is scope for improvement in terms of the model itself as well the the rewards structure for the environment. Please find the detailed analysis in the attached report."""